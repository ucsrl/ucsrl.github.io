# Boring Publications

This short essay is a response to Lemire's post. 
Originally, I intended for this to be quick tweet, but after line 35 of my response, I realized that it's much too long for Twitter and would be much better served as an essay on its own.

## Tl;dr
Interesting read; I do have some reservations, though. Generally speaking, I see the same symptoms, but think the diagnosis is too simplistic to generalize over all science. 
Instead of the mono-causal need for "customers," I'd attest a melange of a variety of shortcomings, which in combination lead to slow pace: 
- fashion, 
- conferences v. journals, and 
- a good dose of Goodhart's law.

## Opposing view: Need for customers
I am opposing the view that science *needs* to have a customer. There are practical fields, where this clearly is useful and reasonable.\
But there are also fields that are at odds with this requirement. Many subjects have theoretical fields, where there may not be a 
customer available right now. A customer may emerge in 100 or 200 years in the future. To mandate a customer would effectively stifle 
future research endeavors.

## Multi-causal vs mono-causal

Over the past ten years, I've often felt that parts of computer science I am familiar with have become increasingly fashion oriented.
Among the non-CS scientists I know, the feeling is pretty much the same.
The fashion aspect in science is pervasive: it influences choice of topics, relative ranking of topics, being "first," importance of venues, and certainly a lot of other things. 
The key downside of this fashion circus is that it often moves to fast for science to matter:
When one gets past the low-hanging fruit and the fashion circuit moved elsewhere, what are you supposed to do?
CS conferences need reconsideration, too. 
I've always had the opinion that CS is *not* special and that the conference-focus is merely an artifact of its age. 
Conferences also suffer from fashion-aspects, but the primary function of conferences seems to be\
defunct. 
When you take 100pct of the papers, probably half that are not a good fit (premature submission, subpar language, etc.) can be identified relatively quickly. 
But what about the rest? Depending on the acceptance rate, some papers get in, other don't. 
The acceptance rate has become a proxy for selectivity and prestigiousness, but in reality it is often random and more of a social process of the PC members. 
(A couple of experiments seem to confirm the questionable function of PCs [NeurIPS].)
Another big downside of conferences is PC selection, which is also affected by social and sometimes also institutional processes and biases, in combination with scientist tendencies of being "experts" in all subfields of a certain field. 
I have seen the most blatant mistakes from so-called "expert" reviews that are obviously and objectively false. Reviewers are also not accountable for writing bad reviews, meaning that there is no downside to writing bad reviews.
Goodhart's law is also complicit. Scientists are smart people and will figure out a way to game the system. If the number of publications is relevant, they will figure out way to maximize publication count. Replace publications with any kind of goal, then that will be optimized for.
Michael Stonebraker summarized his "fears" for database research a couple of years ago, and I feel his diagnosis generalizes to all systems research. 
I talked to several senior members of the PL community and they confirmed that PLDI before the 00s was a lot more about ideas without a solid, industrial-strength implementation. 
Nowadays, a grad student has to implement something for two years only to be shut down by a reviewer in what often appears to amount to less than one hour of work.
What are we to do? We need an environment conducive to and fostering research. Personally, I believe that we need to stop the nonsensical and gigantic waste of tax payer money to maintain the conference circuit that shows diminishing returns. 
If we had journals, with smaller focus and smaller, but actual expert reviewers, we could ensure progress in *all* subfields regardless of fashion. 
By having two or three journals in a sub-field, one could focus on early-stage/prototypical v late-stage/finished work.
Goodhart's law ensures that this switch is not ideal and will also have downsides. The alternative is to maintain the status quo.
Whether this is wise or not, I do not know. What I do know, though, is that Einstein's famous definition of insanity may be relevant:
"Doing the same thing over and over again and expecting different results." 
